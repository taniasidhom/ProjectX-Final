{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import names\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API Credentials \n",
    "\n",
    "consumer_key= 'Rs2MF8SA8oBOIc8VF6yuTLF36'\n",
    "consumer_secret= 'MF0Q5mn1PRG2jSuD5unaewRkdOnPeWWyd6ihzyK03NvisiAQ3S'\n",
    "access_token= '1321538408246857728-iRZRpLkYPjhqU7O642Cu4pMMYWTx0Y'\n",
    "access_token_secret= 'yb8qSFgKB2TOTgHF5W7X9ZRDS0VzADhVFka8CHfqYDLDH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorization \n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CyberMonday\n",
      "#MondayMotivation\n",
      "#PLTPINKMONDAY\n",
      "#mondaythoughts\n",
      "#MondayMorning\n",
      "St. Andrew\n",
      "Guru Nanak Dev Ji\n",
      "Renaudot\n",
      "Moderna\n",
      "Good Monday\n",
      "Innus\n",
      "Val-d'Or\n",
      "PHSA\n",
      "thorncliffe park public school\n",
      "happy gurpurab\n",
      "Service Canada\n",
      "O'Toole\n",
      "TODAY ONLY\n",
      "GOT7\n",
      "Sikhism\n",
      "Quality Street\n",
      "Super Nintendo World\n",
      "listen live\n",
      "gnl québec\n",
      "SUNWOO\n",
      "Lisa Raitt\n",
      "special weather statement\n",
      "China\n",
      "Shop Now\n",
      "ginella\n",
      "LAST DAY\n",
      "Minassian\n",
      "François Legault\n",
      "oates\n",
      "Book Fair\n",
      "LAST CHANCE\n",
      "Debt\n",
      "Kushner\n",
      "Doom Eternal\n",
      "York Region\n",
      "CBC News\n",
      "youngjae\n",
      "Urdu\n",
      "Silicon Valley\n",
      "Reps\n",
      "Barbados\n",
      "My Best Relationship\n",
      "My Sign\n",
      "SOLD OUT\n",
      "jongin\n"
     ]
    }
   ],
   "source": [
    "#Get top 20 Twitter Trends in Toronto (used in paper)\n",
    "#Returns more recent trends on Monday, Nov. 30th 2020\n",
    "\n",
    "trends_result = api.trends_place(4118)\n",
    "for trend in trends_result[0][\"trends\"]:\n",
    "    print(trend[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashtags(tweet):\n",
    "    hashtag = tweet.entities['hashtags']\n",
    "    res = []\n",
    "    \n",
    "    if len(hashtag) != 0:\n",
    "        for i in range(len(hashtag)):\n",
    "            res.append(tweet.entities['hashtags'][i]['text'])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word): \n",
    "    return {'last_letter':word[-1]} \n",
    "\n",
    "# preparing a list of examples and corresponding class labels. \n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')]+\n",
    "             [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names) \n",
    "\n",
    "# we use the feature extractor to process the names data. \n",
    "featuresets = [(gender_features(n), gender)  \n",
    "               for (n, gender)in labeled_names] \n",
    "\n",
    "# Divide the resulting list of feature \n",
    "# sets into a training set and a test set. \n",
    "train_set, test_set = featuresets[500:], featuresets[:500] \n",
    "\n",
    "# The training set is used to  \n",
    "# train a new \"naive Bayes\" classifier. \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = [\"#BlackFriday\", \"#FastFashion\", \"#SmallBusinessSaturday\", \"#CyberMonday\", \"#SecondHand\", \"#SecondHandClothes\", \"#Sustainability\", \n",
    "            \"#CiruclarFashion\", \"#CircularEconomy\", \"WasteReduction\", '#VintageClothing', \"#Second-HandShopping\", \"#BlueBox\", \"#ThriftShopping\", \"ClothingBin\",\n",
    "            \"#Textiles\", \"#TextileDonation\", \"#TextileDiversion\", '#WasteDiversion', \"#DonationBins\", \"#Re-use\", \"#NetZero\", \"#ClothingSwaps\", \n",
    "            \"#ClothingRentals\", \"#ClimateChange\", \"#Downcycling\", \"#EthicalFashion\", \"#FashionRevolution\", \"#LocallyMade\", \"#Repurposing\", \"#Recycle\", \n",
    "            \"#RecycledClothing\", \"#FashionFootprint\", \"#SustainableFashion\", \"#SustainableFabrics\", \"#SustainableFashionMovement\", \"#SlowFashion\", \n",
    "            \"#Second-HandClothing\", \"#SeasonlessClothing\", \"#SyntheticFibers\", \"#Trashion\", \"#TextileIndustry\", \"#TextileWaste\", \"#ThriftShops\", \n",
    "            \"#Upcycling\", \"#ZeroWaste\", \"#VeganFashion\", \"#EcoFashion\", '#15PercentPledge', '#BuyLess', '#BuyLocal', '#ValueVillage', '#DiabetesCanada', '#FTA', '#FashionTakesAction', \n",
    "            '#ThriftedFashion', '#SecondHandButGrand', '#LovedClothesLast', '#DiscountCulture', '#OverProduction', '#NoBlackFriday', \n",
    "            '#NoSales', '#ApparelWaste', '#TextileRecycling', '#7RsofFashion', '#EthicalCloset', '#SupportTorontoFashion', '#ConsciousConsumerism', \n",
    "            '#FashionImpacts', '#SGDs', \"#SustainableDevelopmentGoals\", \"#MarieKondoMethod\", \"#CleanOutYourCloset\", \"#MakeFashionCircular\", \n",
    "            \"#SalvationArmy\", \"#SalvationArmyToronto\", \"NACTR\", \"PudoPoints\", \"#GiveBackCanada\"]\n",
    "\n",
    "keywords = [\"Black Friday\", \"Fast Fashion\", \"Small Business Saturday\", \"Cyber Monday\", \"Second Hand\", \"Second Hand Clothes\", \"Sustainability\", \n",
    "            \"Ciruclar Fashion\", \"Circular Economy\", \"Waste Reduction\", 'Vintage Clothing', \"Second-Hand Shopping\", \"Blue Box\", \"Thrift Shopping\", \"Clothing Bin\",\n",
    "            \"Textiles\", \"Textile Donation\", \"Textile Diversion\", 'Waste Diversion', \"Donation Bins\", \"Re-use\", \"NetZero\", \"Clothing Swaps\", \n",
    "            \"Clothing Rentals\", \"Climate Change\", \"Downcycling\", \"Ethical Fashion\", \"Fashion Revolution\", \"Locally Made\", \"Repurposing\", \"Recycle\", \n",
    "            \"Recycled Clothing\", \"Fashion Footprint\", \"Sustainable Fashion\", \"Sustainable Fabrics\", \"Sustainable Fashion Movement\", \"#SlowFashion\", \n",
    "            \"Second-Hand Clothing\", \"Seasonless Clothing\", \"Synthetic Fibers\", \"Trashion\", \"Textile Industry\", \"Textile Waste\", \"Thrift Shops\", \n",
    "            \"Upcycling\", \"Zero Waste\", \"Vegan Fashion\", \"Eco Fashion\", '15 Percent Pledge', 'Buy Less', 'Buy Local', 'Value Village', 'Diabetes Canada', 'FTA', 'Fashion Takes Action', \n",
    "            'Thrifted Fashion', 'SecondHand But Grand', 'Loved Clothes Last', 'Discount Culture', 'Over Production', 'No Black Friday', \n",
    "            'No Sales', 'Apparel Waste', 'Textile Recycling', '7Rs of Fashion', 'Ethical Closet', 'Support Toronto Fashion', 'Conscious Consumerism', \n",
    "            'Fashion Impacts', 'SGDs', \"Sustainable Development Goals\", \"Marie Kondo Method\", \"Clean Out Your Closet\", \"Make Fashion Circular\", \n",
    "            \"Salvation Army\", \"Salvation Army Toronto\", \"NACTR\", \"Pudo Points\", \"GiveBack Canada\"]\n",
    "\n",
    "\n",
    "def info(searchQuery, word, start_date, end_date):\n",
    "    data = []\n",
    "    tweets = tweepy.Cursor(api.search,\n",
    "                           geocode='43.70011,-79.4163,70km', \n",
    "                           q=searchQuery, \n",
    "                           lang=\"en\", \n",
    "                           since=start_date, \n",
    "                           until=end_date).items(1000)\n",
    "    for tweet in tweets:\n",
    "        level = []\n",
    "        level.append(searchQuery)\n",
    "        level.append(tweet.text)\n",
    "        level.append(tweet.created_at)\n",
    "        level.append(tweet.user.id_str)\n",
    "        level.append(classifier.classify(gender_features(str(tweet.user.name.split()[0]))))\n",
    "        level.append(tweet.user.location)\n",
    "        level.append(getHashtags(tweet))\n",
    "        level.append(tweet.user.followers_count)\n",
    "        level.append(tweet.retweet_count)\n",
    "        level.append(tweet.favorite_count)\n",
    "        data.append(level)\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['{}'.format(word),\n",
    "                                     'tweet', \n",
    "                                     'timestamp', \n",
    "                                     'user_ID', \n",
    "                                     'user_gender',\n",
    "                                     'user_location',\n",
    "                                     'hashtags',\n",
    "                                     'num_followers',\n",
    "                                     'num_retweets',\n",
    "                                     'num_favourites'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = []\n",
    "for hashtag in hashtags:\n",
    "    df1 = info(keyword, 'keyword', \"2020-11-23\", \"2020-11-30\")\n",
    "    tweets1.append(df1)\n",
    "tweets_by_keyword = pd.concat(tweets1)\n",
    "\n",
    "tweets2 = []\n",
    "for keyword in keywords:\n",
    "    df2 = info(keyword, 'keyword', \"2020-11-23\", \"2020-11-30\")\n",
    "    tweets1.append(df2)\n",
    "tweets_by_keyword = pd.concat(tweets2)\n",
    "\n",
    "#save to csv\n",
    "tweets_by_keyword.to_csv('tweets_by_keyword.csv', index=False)\n",
    "tweets_by_hashtag.to_csv('tweets_by_hashtag.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
